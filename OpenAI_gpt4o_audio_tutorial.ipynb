{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af26d53",
   "metadata": {},
   "source": [
    "# Unlocking Audio Capabilities with OpenAI's gpt-4o-audio-preview Model: A Practical Guide\n",
    "    \n",
    "This notebook will walk you through how to use OpenAI’s new `gpt-4o-audio-preview` model using LangChain.\n",
    "We’ll go step by step, covering everything from environment setup to audio processing and advanced use cases like tool calling and chaining tasks.\n",
    "\n",
    "Whether you want to transcribe audio or generate spoken responses, this guide will get you up and running with practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we get started, ensure you have:\n",
    "- An OpenAI account and API key.\n",
    "- The `langchain-openai` package installed.\n",
    "- (Optional) LangSmith for tracing your API calls.\n",
    "\n",
    "Let's begin by setting up our environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9f3a4",
   "metadata": {},
   "source": [
    "### 1. Installing the Required Packages\n",
    "\n",
    "We’ll need the `langchain-openai` package to interact with OpenAI models. Run the command below to install it.\n",
    "\n",
    "```bash\n",
    "%pip install -qU langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ee243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain-openai package\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03bf53",
   "metadata": {},
   "source": [
    "### 2. Setting Up Environment Variables\n",
    "\n",
    "Now we’ll set up the environment variables to store your OpenAI API key. This keeps sensitive information out of your code.\n",
    "\n",
    "You can manually set the environment variable in your terminal, or use a `.env` file in combination with `python-dotenv`. Here, we’ll show you how to set it within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea87b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84d3ba",
   "metadata": {},
   "source": [
    "### 3. Instantiating the Model\n",
    "\n",
    "Now that we have our environment set up, let’s instantiate the `gpt-4o-audio-preview` model using LangChain. We’ll also configure some basic parameters like temperature and token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Instantiate the model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-audio-preview\",  # Specifying the model\n",
    "    temperature=0,  # Low randomness for structured output\n",
    "    max_tokens=None,  # Unlimited tokens (set a limit if needed)\n",
    "    timeout=None,  # No timeout for processing\n",
    "    max_retries=2  # Retry if the request fails\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802922ef",
   "metadata": {},
   "source": [
    "### 4. Uploading and Encoding Audio Files\n",
    "\n",
    "We’ll now upload an audio file and encode it into base64 format so that it can be processed by the model. Here’s how you can read and encode an audio file in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Open the audio file and convert to base64\n",
    "with open(\"path/to/audio.wav\", \"rb\") as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "# Convert binary audio data to base64\n",
    "audio_b64 = base64.b64encode(audio_data).decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a7a23",
   "metadata": {},
   "source": [
    "### 5. Transcribing Audio\n",
    "\n",
    "Now that we’ve encoded the audio, we can pass it to the model and get a transcription. Let’s send the request and retrieve the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98972ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send audio for transcription\n",
    "messages = [\n",
    "    (\n",
    "        \"human\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": \"Transcribe the following:\"},\n",
    "            {\"type\": \"input_audio\", \"input_audio\": {\"data\": audio_b64, \"format\": \"wav\"}},\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# Invoke the model and get the transcription\n",
    "output_message = llm.invoke(messages)\n",
    "print(output_message.content)  # The transcription will appear here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae0df6",
   "metadata": {},
   "source": [
    "### 6. Generating Audio Responses\n",
    "\n",
    "Let’s now configure the model to generate audio outputs, allowing it to respond with actual speech. We’ll specify the voice and format for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8971f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model to generate audio responses\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-audio-preview\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\n",
    "        \"modalities\": [\"text\", \"audio\"],  # Enable audio responses\n",
    "        \"audio\": {\"voice\": \"alloy\", \"format\": \"wav\"},  # Set voice and output format\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate a response with audio\n",
    "messages = [(\"human\", \"Are you made by OpenAI? Just answer yes or no.\")]\n",
    "output_message = llm.invoke(messages)\n",
    "\n",
    "# Access the generated audio data\n",
    "audio_response = output_message.additional_kwargs['audio']['data']\n",
    "print(f\"Generated audio (base64): {audio_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552238e",
   "metadata": {},
   "source": [
    "### 7. Saving and Playing Back Audio\n",
    "\n",
    "After generating the audio response, you might want to save it and play it back. Here’s how you can decode the base64-encoded audio data and save it as a `.wav` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the base64 audio data\n",
    "audio_bytes = base64.b64decode(audio_response)\n",
    "\n",
    "# Save the audio as a .wav file\n",
    "with open(\"output.wav\", \"wb\") as f:\n",
    "    f.write(audio_bytes)\n",
    "\n",
    "print(\"Audio saved as output.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06892fc0",
   "metadata": {},
   "source": [
    "### 8. Tool Binding and Task Chaining\n",
    "\n",
    "In more advanced use cases, you can bind tools to the model and chain tasks. For example, we can bind a weather fetching tool to the model and chain it with transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd798784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a tool to fetch weather information\n",
    "class GetWeather(BaseModel):\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "# Bind the tool to the model\n",
    "llm_with_tools = llm.bind_tools([GetWeather])\n",
    "\n",
    "# Use the bound tool to fetch weather data\n",
    "ai_msg = llm_with_tools.invoke(\"What's the weather like in San Francisco, CA?\")\n",
    "print(ai_msg)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
